{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-28T05:48:51.329278Z",
     "start_time": "2023-12-28T05:48:51.324676Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "logs_path = Path(\"logs\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T05:49:02.281579Z",
     "start_time": "2023-12-28T05:49:02.270423Z"
    }
   },
   "id": "4d28455235e79416",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "log_files = sorted([f for f in logs_path.iterdir() if f.is_file() and f.suffix == \".log\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T05:58:50.799445Z",
     "start_time": "2023-12-28T05:58:50.795288Z"
    }
   },
   "id": "d946091f21183ca0",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "main_program_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}) \\| INFO     \\| __main__:run_evaluation:42 - Models: (.*) \\| Tasks: (.*) \\| Prompting: (.*)\")\n",
    "loop_combination_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}) \\| INFO     \\| __main__:run_evaluation:78 - Running (.*) on (.*) with (.*)\")\n",
    "model_start_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}) \\| INFO     \\| __main__:run_evaluation:106 - Running (.*)\")\n",
    "skip_evaluation_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}) \\| INFO     \\| __main__:run_evaluation:110 - Skipping (\\d+)\") # OR\n",
    "start_prompt_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}) \\| DEBUG    \\| __main__:run_evaluation:116 - (.*)\")\n",
    "inference_start_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}) \\| INFO     \\| src.llms.(?:.*):inference:17 - Generating response from (.*)\")\n",
    "\n",
    "no_response_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}) \\| DEBUG    \\| src.llms.(?:.*):inference:23 - No response generated, returning empty string\") # OR\n",
    "start_answer_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}) \\| DEBUG    \\| src.llms.(?:.*):inference:25 - (.*)\")\n",
    "metadata_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+) \\| SUCCESS  \\| src.llms.(?:.*):inference:26 - Response generated from (.*)\")\n",
    "\n",
    "failed_extraction_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+) \\| DEBUG    \\| src.tasks.(?:.*):evaluate:76 - Could not extract prediction from response\") # OR\n",
    "failed_extraction_empty_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+) \\| DEBUG    \\| src.tasks.(?:.*):evaluate:55 - Could not extract prediction from response as response is empty\")\n",
    "prediction_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+) \\| DEBUG    \\| src.tasks.(?:.*):evaluate:74 - Prediction: (.*), Answer: (.*)\")\n",
    "\n",
    "current_progress_pattern = re.compile(r\"(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d+) \\| INFO     \\| __main__:run_evaluation:140 - Correct: (\\d+)/(\\d+) \\((\\d+\\.\\d+)%\\)\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T06:39:04.974886Z",
     "start_time": "2023-12-28T06:39:04.969320Z"
    }
   },
   "id": "4cc1ba6efc7759f8",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Main match\n",
      "=> Loop match\n",
      "=> [Model start match] 2023-11-19 15:41:54.189 | INFO     | __main__:run_evaluation:106 - Running palm-2-chat\n",
      "=> [Prompt match] 2023-11-19 15:41:54.189 | DEBUG    | __main__:run_evaluation:116 - Question: A car is being driven, in a straight line and at a uniform speed, towards the base of a vertical tower. The top of the tower is observed from the car and, in the process, it takes 10 minutes for the angle of elevation to change from 45° to 60°. After how much more time will this car reach the base of the tower?\n",
      "=> [ADD] Choices: A)5(√3 + 1), B)6(√3 + √2), C)7(√3 – 1), D)8(√3 – 2), E)None of these\n",
      "=> [ADD] Answer:\n",
      "=> [Inference match] 2023-11-19 15:41:54.189 | INFO     | src.llms.palm_two_chat:inference:17 - Generating response from PaLM 2 Chat\n",
      "=> [Answer match] 2023-11-19 15:42:02.446 | DEBUG    | src.llms.palm_two_chat:inference:25 - The answer is $\\boxed{5\\sqrt{3}}$ minutes.\n",
      "=> [ADD] \n",
      "=> [ADD] Let $x$ be the distance from the car to the base of the tower at the start of the 10 minutes. Let $y$ be the distance from the car to the top of the tower at the start of the 10 minutes. Let $z$ be the distance from the car to the top of the tower at the end of the 10 minutes.\n",
      "=> [ADD] \n",
      "=> [ADD] We know that the angle of elevation to the top of the tower changes from 45° to 60°. This means that the tangent of the angle of elevation changes from $\\tan(45^\\circ) = \\sqrt{3}$ to $\\tan(60^\\circ) = \\sqrt{3}/2$.\n",
      "=> [ADD] \n",
      "=> [ADD] We also know that the distance from the car to the base of the tower is increasing at a uniform rate. This means that the rate of change of the distance from the car to the base of the tower is constant.\n",
      "=> [ADD] \n",
      "=> [ADD] We can use this information to set up the following equation:\n",
      "=> [ADD] \n",
      "=> [ADD] $$\\frac{dy}{dx} = \\sqrt{3}$$\n",
      "=> [ADD] \n",
      "=> [ADD] We can integrate both sides of this equation to get:\n",
      "=> [ADD] \n",
      "=> [ADD] $$y = \\sqrt{3}x + C$$\n",
      "=> [ADD] \n",
      "=> [ADD] We know that $y = 0$ when $x = 0$. We can use this information to solve for $C$:\n",
      "=> [ADD] \n",
      "=> [ADD] $$0 = \\sqrt{3}(0) + C$$\n",
      "=> [ADD] \n",
      "=> [ADD] $$C = 0$$\n",
      "=> [ADD] \n",
      "=> [ADD] We can now plug in $x = 5$ to get:\n",
      "=> [ADD] \n",
      "=> [ADD] $$y = \\sqrt{3}(5) + 0 = 5\\sqrt{3}$$\n",
      "=> [ADD] \n",
      "=> [ADD] This means that the car is 5$\\sqrt{3}$ minutes away from the base of the tower at the end of the 10 minutes.\n",
      "=> [ADD] \n",
      "=> [ADD] Therefore, the car will reach the base of the tower in 5$\\sqrt{3}$ minutes more.\n",
      "=> [Metadata match] 2023-11-19 15:42:02.447 | SUCCESS  | src.llms.palm_two_chat:inference:26 - Response generated from PaLM 2 Chat, response length: 1294, time taken: 8.25709800000186 seconds\n",
      "=> [Failed extraction match] 2023-11-19 15:42:02.447 | DEBUG    | src.tasks.aqua:evaluate:76 - Could not extract prediction from response\n",
      "=> [Progress match] 2023-11-19 15:42:02.449 | INFO     | __main__:run_evaluation:140 - Correct: 0/1 (0.00%)\n",
      "=> [Model start match] 2023-11-19 15:42:02.450 | INFO     | __main__:run_evaluation:106 - Running palm-2-chat\n",
      "=> [Prompt match] 2023-11-19 15:42:02.450 | DEBUG    | __main__:run_evaluation:116 - Question: The original price of an item is discounted 22%. A customer buys the item at this discounted price using a $20-off coupon. There is no tax on the item, and this was the only item the customer bought. If the customer paid $1.90 more than half the original price of the item, what was the original price of the item?\n",
      "=> [ADD] Choices: A)$61, B)$65, C)$67.40, D)$70, E)$78.20\n",
      "=> [ADD] Answer:\n",
      "=> [Inference match] 2023-11-19 15:42:02.450 | INFO     | src.llms.palm_two_chat:inference:17 - Generating response from PaLM 2 Chat\n",
      "=> [Answer match] 2023-11-19 15:42:05.735 | DEBUG    | src.llms.palm_two_chat:inference:25 - Let $x$ be the original price of the item. After the 22% discount, the price of the item is $0.78x$. After the $20$-off coupon, the price of the item is $0.78x - 20$. The customer paid $1.90 more than half the original price, so the price they paid is $1.90 + \\frac{1}{2}x$. We know that $1.90 + \\frac{1}{2}x = 0.78x - 20$. Solving for $x$, we get $x = \\boxed{\\$65}$.\n",
      "=> [Metadata match] 2023-11-19 15:42:05.736 | SUCCESS  | src.llms.palm_two_chat:inference:26 - Response generated from PaLM 2 Chat, response length: 367, time taken: 3.2852625839877874 seconds\n",
      "=> [Failed extraction match] 2023-11-19 15:42:05.736 | DEBUG    | src.tasks.aqua:evaluate:76 - Could not extract prediction from response\n",
      "=> [Progress match] 2023-11-19 15:42:05.737 | INFO     | __main__:run_evaluation:140 - Correct: 0/2 (0.00%)\n",
      "=> [Model start match] 2023-11-19 15:42:05.738 | INFO     | __main__:run_evaluation:106 - Running palm-2-chat\n",
      "=> [Prompt match] 2023-11-19 15:42:05.738 | DEBUG    | __main__:run_evaluation:116 - Question: Find out which of the following values is the multiple of X, if it is divisible by 9 and 12?\n",
      "=> [ADD] Choices: A)36, B)15, C)17, D)5, E)7\n",
      "=> [ADD] Answer:\n",
      "=> [Inference match] 2023-11-19 15:42:05.738 | INFO     | src.llms.palm_two_chat:inference:17 - Generating response from PaLM 2 Chat\n",
      "=> [No response match] 2023-11-19 15:42:09.278 | DEBUG    | src.llms.palm_two_chat:inference:23 - No response generated, returning empty string\n",
      "=> [Answer match] 2023-11-19 15:42:09.279 | DEBUG    | src.llms.palm_two_chat:inference:25 - \n",
      "=> [Metadata match] 2023-11-19 15:42:09.279 | SUCCESS  | src.llms.palm_two_chat:inference:26 - Response generated from PaLM 2 Chat, response length: 0, time taken: 3.539702624984784 seconds\n",
      "=> [Failed extraction empty match] 2023-11-19 15:42:09.279 | DEBUG    | src.tasks.aqua:evaluate:55 - Could not extract prediction from response as response is empty\n",
      "=> [Progress match] 2023-11-19 15:42:09.280 | INFO     | __main__:run_evaluation:140 - Correct: 0/3 (0.00%)\n",
      "=> [Model start match] 2023-11-19 15:42:09.280 | INFO     | __main__:run_evaluation:106 - Running palm-2-chat\n",
      "=> [Prompt match] 2023-11-19 15:42:09.280 | DEBUG    | __main__:run_evaluation:116 - Question: If the probability that Stock A will increase in value during the next month is 0.56, and the probability that Stock B will increase in value during the next month is 0.74. What is the greatest value for the probability that neither of these two events will occur?\n",
      "=> [ADD] Choices: A)0.22, B)0.26, C)0.37, D)0.46, E)0.63\n",
      "=> [ADD] Answer:\n",
      "=> [Inference match] 2023-11-19 15:42:09.281 | INFO     | src.llms.palm_two_chat:inference:17 - Generating response from PaLM 2 Chat\n",
      "=> [Answer match] 2023-11-19 15:42:11.941 | DEBUG    | src.llms.palm_two_chat:inference:25 - The probability of two events occurring is the product of their probabilities. So, the probability that Stock A will increase in value and Stock B will increase in value is 0.56 * 0.74 = 0.4224. The probability that neither of these two events will occur is 1 - 0.4224 = 0.5776. Therefore, the greatest value for the probability that neither of these two events will occur is $\\boxed{\\text{C) 0.57}}$.\n",
      "=> [Metadata match] 2023-11-19 15:42:11.941 | SUCCESS  | src.llms.palm_two_chat:inference:26 - Response generated from PaLM 2 Chat, response length: 401, time taken: 2.6603358749998733 seconds\n",
      "=> [Prediction match] 2023-11-19 15:42:11.941 | DEBUG    | src.tasks.aqua:evaluate:74 - Prediction: C, Answer: B\n",
      "=> [Progress match] 2023-11-19 15:42:11.942 | INFO     | __main__:run_evaluation:140 - Correct: 0/4 (0.00%)\n",
      "=> [Model start match] 2023-11-19 15:42:11.942 | INFO     | __main__:run_evaluation:106 - Running palm-2-chat\n",
      "=> [Prompt match] 2023-11-19 15:42:11.942 | DEBUG    | __main__:run_evaluation:116 - Question: A trader sold an article at a profit of 20% for Rs.360. What is the cost price of the article?\n",
      "=> [ADD] Choices: A)270, B)300, C)280, D)320, E)315\n",
      "=> [ADD] Answer:\n",
      "=> [Inference match] 2023-11-19 15:42:11.942 | INFO     | src.llms.palm_two_chat:inference:17 - Generating response from PaLM 2 Chat\n",
      "=> [Answer match] 2023-11-19 15:42:14.603 | DEBUG    | src.llms.palm_two_chat:inference:25 - The answer is B)300.\n",
      "=> [ADD] \n",
      "=> [ADD] Let $x$ be the cost price of the article. The profit is 20% of the cost price, so the selling price is $x + 0.2x = 1.2x$. We are given that the selling price is Rs.360, so $1.2x = 360$. Solving for $x$, we get $x = 300$.\n",
      "=> [Metadata match] 2023-11-19 15:42:14.603 | SUCCESS  | src.llms.palm_two_chat:inference:26 - Response generated from PaLM 2 Chat, response length: 242, time taken: 2.6606962920050137 seconds\n",
      "=> [Prediction match] 2023-11-19 15:42:14.604 | DEBUG    | src.tasks.aqua:evaluate:74 - Prediction: B, Answer: B\n",
      "=> [Progress match] 2023-11-19 15:42:14.606 | INFO     | __main__:run_evaluation:140 - Correct: 1/5 (20.00%)\n",
      "=> [Model start match] 2023-11-19 15:42:14.606 | INFO     | __main__:run_evaluation:106 - Running palm-2-chat\n",
      "=> [Prompt match] 2023-11-19 15:42:14.607 | DEBUG    | __main__:run_evaluation:116 - Question: 20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in. Then, another 20 marbles were pulled out, of which 1 was black, after which they were all returned to the bag. If the percentage of black marbles pulled out the second time represents their percentage in the bag, how many marbles in total Q does the bag currently hold?\n",
      "=> [ADD] Choices: A)40, B)200, C)380, D)400, E)3200\n",
      "=> [ADD] Answer:\n",
      "=> [Inference match] 2023-11-19 15:42:14.607 | INFO     | src.llms.palm_two_chat:inference:17 - Generating response from PaLM 2 Chat\n",
      "=> [Answer match] 2023-11-19 15:42:18.486 | DEBUG    | src.llms.palm_two_chat:inference:25 - The answer is $\\boxed{3200}$.\n",
      "=> [ADD] \n",
      "=> [ADD] Let $x$ be the total number of marbles in the bag. The first time 20 marbles were pulled out, 1 of them was black. This means that the percentage of black marbles in the bag is $\\frac{1}{20} = \\frac{1}{20}x$. The second time 20 marbles were pulled out, 1 of them was black. This means that the percentage of black marbles in the bag is still $\\frac{1}{20}x$. Since the percentage of black marbles pulled out the second time represents their percentage in the bag, we know that $\\frac{1}{20}x = \\frac{1}{20}Q$, where $Q$ is the total number of marbles in the bag. Solving for $Q$, we get $Q = 3200$.\n",
      "=> [Metadata match] 2023-11-19 15:42:18.486 | SUCCESS  | src.llms.palm_two_chat:inference:26 - Response generated from PaLM 2 Chat, response length: 629, time taken: 3.8787522499915212 seconds\n",
      "=> [Failed extraction match] 2023-11-19 15:42:18.486 | DEBUG    | src.tasks.aqua:evaluate:76 - Could not extract prediction from response\n",
      "=> [Progress match] 2023-11-19 15:42:18.487 | INFO     | __main__:run_evaluation:140 - Correct: 1/6 (16.67%)\n",
      "=> [Model start match] 2023-11-19 15:42:18.488 | INFO     | __main__:run_evaluation:106 - Running palm-2-chat\n",
      "=> [Prompt match] 2023-11-19 15:42:18.488 | DEBUG    | __main__:run_evaluation:116 - Question: Find the total no. of distinct bike no.'s that can beformed using 2 letters followed by 2 no.'s. How many letters need to be distinct?\n",
      "=> [ADD] Choices: A)74453, B)64543, C)74325, D)65000, E)97656\n",
      "=> [ADD] Answer:\n",
      "=> [Inference match] 2023-11-19 15:42:18.489 | INFO     | src.llms.palm_two_chat:inference:17 - Generating response from PaLM 2 Chat\n",
      "=> [Answer match] 2023-11-19 15:42:26.483 | DEBUG    | src.llms.palm_two_chat:inference:25 - The total number of distinct bike numbers that can be formed using 2 letters followed by 2 numbers is 97656. This is because there are 26 letters in the alphabet and 10 numbers, so there are $26 \\times 26 \\times 10 \\times 10 = 676000$ possible combinations. However, some of these combinations will be the same, for example \"AA11\" and \"BB11\" are both the same number. To account for this, we need to divide the total number of combinations by the number of ways to order the letters and numbers, which is 2!2! = 24. This gives us $676000 \\div 24 = 28571$. Finally, we need to subtract 1 from this number because the number \"00\" is not a valid bike number. This gives us the final answer of 28570.\n",
      "=> [ADD] \n",
      "=> [ADD] To answer the question of how many letters need to be distinct, we can see that there are 26 possible combinations for the first letter, 25 possible combinations for the second letter, 10 possible combinations for the first number, and 9 possible combinations for the second number. This gives us a total of $26 \\times 25 \\times 10 \\times 9 = 576500$ possible combinations. However, some of these combinations will be the same, for example \"AA11\" and \"BB11\" are both the same number. To account for this, we need to divide the total number of combinations by the number of ways to order the letters and numbers, which is 2!2! = 24. This gives us $576500 \\div 24 = 23638$. Finally, we need to subtract 1 from this number because the number \"00\" is not a valid bike number. This gives us the final answer of 23637. Therefore, at least 3 letters need to be distinct in order to form a unique bike number.\n",
      "=> [Metadata match] 2023-11-19 15:42:26.483 | SUCCESS  | src.llms.palm_two_chat:inference:26 - Response generated from PaLM 2 Chat, response length: 1599, time taken: 7.993709542002762 seconds\n",
      "=> [Failed extraction match] 2023-11-19 15:42:26.483 | DEBUG    | src.tasks.aqua:evaluate:76 - Could not extract prediction from response\n",
      "=> [Progress match] 2023-11-19 15:42:26.485 | INFO     | __main__:run_evaluation:140 - Correct: 1/7 (14.29%)\n"
     ]
    }
   ],
   "source": [
    "with open(log_files[0], \"r\") as f:\n",
    "    log_lines = f.read().splitlines()\n",
    "    current_model = None\n",
    "    current_task = None\n",
    "    current_prompting = None\n",
    "    current_content = None\n",
    "    in_content_loop = False\n",
    "    obj = {\n",
    "        \"prompt\": None,\n",
    "        \n",
    "    }\n",
    "    counter = 0\n",
    "    for line in log_lines[:100]:\n",
    "        main_match = main_program_pattern.match(line)\n",
    "        loop_match = loop_combination_pattern.match(line)\n",
    "        if main_match:\n",
    "            print(\"=> Main match\")\n",
    "            log_time, models, tasks, prompting = main_match.groups()\n",
    "            models = models.split(\", \")\n",
    "            tasks = tasks.split(\", \")\n",
    "            prompting = prompting.split(\", \")\n",
    "        elif loop_match:\n",
    "            print(\"=> Loop match\")\n",
    "            log_time, model, task, prompting = loop_match.groups()\n",
    "            current_model = model\n",
    "            current_task = task\n",
    "            current_prompting = prompting\n",
    "        elif model_start_pattern.match(line):\n",
    "            print(\"=> [Model start match] \" + line)\n",
    "            log_time, model = model_start_pattern.match(line).groups()\n",
    "            current_model = model\n",
    "            is_current_a_prompt = False\n",
    "            is_current_a_response = False\n",
    "            # TODO: Flush content to file + current_content.strip() + reset current_content if any\n",
    "            # Add one to counter after flushing\n",
    "        elif skip_evaluation_pattern.match(line):\n",
    "            pass\n",
    "        elif start_prompt_pattern.match(line):\n",
    "            print(\"=> [Prompt match] \" + line)\n",
    "            log_time, content = start_prompt_pattern.match(line).groups()\n",
    "            current_content = content + \"\\n\"\n",
    "            in_content_loop = True\n",
    "            is_current_a_prompt = True\n",
    "        elif inference_start_pattern.match(line):\n",
    "            print(\"=> [Inference match] \" + line)\n",
    "            log_time, model = inference_start_pattern.match(line).groups()\n",
    "            in_content_loop = False\n",
    "            is_current_a_prompt = False\n",
    "            # TODO: Flush content to file + current_content.strip() + reset current_content if any\n",
    "        elif start_answer_pattern.match(line):\n",
    "            print(\"=> [Answer match] \" + line)\n",
    "            log_time, content = start_answer_pattern.match(line).groups()\n",
    "            current_content = content + \"\\n\"\n",
    "            in_content_loop = True\n",
    "            is_current_a_response = True\n",
    "        elif metadata_pattern.match(line):\n",
    "            print(\"=> [Metadata match] \" + line)\n",
    "            log_time, model = metadata_pattern.match(line).groups()\n",
    "            in_content_loop = False\n",
    "            is_current_a_response = False\n",
    "            # TODO: Flush content to file + current_content.strip() + reset current_content if any\n",
    "        elif no_response_pattern.match(line):\n",
    "            print(\"=> [No response match] \" + line)\n",
    "            log_time = no_response_pattern.match(line).groups()[0]\n",
    "            in_content_loop = False\n",
    "            is_current_a_response = False\n",
    "        elif failed_extraction_pattern.match(line):\n",
    "            print(\"=> [Failed extraction match] \" + line)\n",
    "            log_time = failed_extraction_pattern.match(line).groups()[0]\n",
    "            in_content_loop = False\n",
    "            is_current_a_response = False\n",
    "        elif failed_extraction_empty_pattern.match(line):\n",
    "            print(\"=> [Failed extraction empty match] \" + line)\n",
    "            log_time = failed_extraction_empty_pattern.match(line).groups()[0]\n",
    "            in_content_loop = False\n",
    "            is_current_a_response = False\n",
    "        elif prediction_pattern.match(line):\n",
    "            print(\"=> [Prediction match] \" + line)\n",
    "            log_time, prediction, answer = prediction_pattern.match(line).groups()\n",
    "            in_content_loop = False\n",
    "            is_current_a_response = False\n",
    "        elif current_progress_pattern.match(line):\n",
    "            print(\"=> [Progress match] \" + line)\n",
    "            log_time, correct, total, percentage = current_progress_pattern.match(line).groups()\n",
    "            in_content_loop = False\n",
    "            is_current_a_response = False\n",
    "        elif in_content_loop:\n",
    "            print(\"=> [ADD] \" + line)\n",
    "            current_content += line + \"\\n\"\n",
    "        else:\n",
    "            raise Exception(\"Unknown line: \" + line)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T06:39:05.179911Z",
     "start_time": "2023-12-28T06:39:05.174004Z"
    }
   },
   "id": "9ee58d6cdfd8575b",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b8aac8ba738b7d82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
